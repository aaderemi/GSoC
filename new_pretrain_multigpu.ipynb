{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaderemi/GSoC/blob/main/new_pretrain_multigpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b52d2df1-c362-4042-9a02-9d8b1895321c",
        "_uuid": "42a014c4-588c-4832-9cc5-c0bc7f635294",
        "id": "6Vd5ccxW6yFn",
        "trusted": true
      },
      "source": [
        "Please set an output directory to store model and other artifacts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install mne\n",
        "#!pip install accelerate -U\n",
        "#!unzip test_code.zip -d test_code\n",
        "#!pip install wandb"
      ],
      "metadata": {
        "id": "40iA1cbBrfX7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "48c409f8-cffe-4692-9636-3bcddd66b37b",
        "_uuid": "ff0e47ee-a287-43a9-a31f-653d5fa95de0",
        "execution": {
          "iopub.execute_input": "2024-07-06T13:06:09.479278Z",
          "iopub.status.busy": "2024-07-06T13:06:09.478940Z",
          "iopub.status.idle": "2024-07-06T13:06:14.121094Z",
          "shell.execute_reply": "2024-07-06T13:06:14.120173Z",
          "shell.execute_reply.started": "2024-07-06T13:06:09.479249Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "lR8WslXTrP14"
      },
      "outputs": [],
      "source": [
        "#automatically creates a config file for training\n",
        "from accelerate.utils import write_basic_config\n",
        "write_basic_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d1f400c-ebd2-44e8-b876-3c973591461f",
        "_uuid": "f523054b-ef8b-45e0-a9cc-fcb83e10721f",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "Nky4hJZHrP18"
      },
      "outputs": [],
      "source": [
        "#session needs to be restarted after calling the above\n",
        "import os\n",
        "os._exit(00)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir output"
      ],
      "metadata": {
        "id": "FyIlc2NVsldz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "1jTYaCXXGehb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8dd84dec-0a5d-4ffe-8fff-bcea395396bc",
        "_uuid": "f080828d-5750-471e-9f49-07ded76736ba",
        "execution": {
          "iopub.execute_input": "2024-07-06T13:06:59.533203Z",
          "iopub.status.busy": "2024-07-06T13:06:59.532321Z",
          "iopub.status.idle": "2024-07-06T13:06:59.594627Z",
          "shell.execute_reply": "2024-07-06T13:06:59.593542Z",
          "shell.execute_reply.started": "2024-07-06T13:06:59.533160Z"
        },
        "id": "GCzqWohr6xsn",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def main(num_epochs, bs, seed):\n",
        "\n",
        "    import os\n",
        "    from rich import print\n",
        "    import mne\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    from accelerate.utils import tqdm\n",
        "    from accelerate.utils import set_seed\n",
        "    from torch.utils.data import DataLoader\n",
        "    from accelerate import DistributedDataParallelKwargs\n",
        "    import wandb\n",
        "\n",
        "    from transformers import Wav2Vec2Config, Wav2Vec2ForPreTraining\n",
        "    from accelerate import Accelerator\n",
        "    from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
        "        ACT2FN,\n",
        "        Wav2Vec2FeatureEncoder,\n",
        "        _compute_mask_indices,\n",
        "        _sample_negative_indices,\n",
        "        Wav2Vec2GroupNormConvLayer,\n",
        "        Wav2Vec2GumbelVectorQuantizer,\n",
        "        Wav2Vec2LayerNormConvLayer,\n",
        "        Wav2Vec2NoLayerNormConvLayer\n",
        "    )\n",
        "\n",
        "    # Initializing a Wav2Vec2 facebook/wav2vec2-base-960h style configuration\n",
        "    configuration = Wav2Vec2Config.from_pretrained(\"facebook/wav2vec2-large\")\n",
        "    model = Wav2Vec2ForPreTraining.from_pretrained(\"facebook/wav2vec2-large\", config=configuration)\n",
        "\n",
        "    configuration.conv_kernel = [3, 2, 2, 2, 2, 2]\n",
        "    configuration.conv_stride = [3, 2, 2, 2, 2, 2]\n",
        "    configuration.conv_dim = [512, 512, 512, 512, 512, 512]\n",
        "    configuration.num_feat_extract_layers = 6\n",
        "    configuration.num_channels = 20\n",
        "    configuration.num_negatives = 20\n",
        "    configuration.mask_time_prob = 0.065\n",
        "    configuration.mask_time_length = 10\n",
        "\n",
        "    # configuration.diversity_loss_weight = 0.0\n",
        "\n",
        "    # set some training arguments here\n",
        "    # save steps and log steps control saving and printing output respectively\n",
        "    save_steps = 10\n",
        "    logging_steps = 1\n",
        "    learning_rate = 1e-2\n",
        "    weight_decay = 0.01\n",
        "    warmup_ratio = 0.05\n",
        "\n",
        "    remove_quantization = False  # set to true to remove quantization\n",
        "    # put output directory for models here\n",
        "    output_dir = \"output\"\n",
        "    # \"/data/work/zeydabadi/test/\"\n",
        "    processed_folder = \"/content/test_code\"\n",
        "\n",
        "    class Wav2Vec2LayerNormConvLayer(Wav2Vec2LayerNormConvLayer):\n",
        "        def __init__(self, config, layer_id=0):\n",
        "            super().__init__(config, layer_id)\n",
        "\n",
        "            self.in_conv_dim = config.conv_dim[layer_id -\n",
        "                                               1] if layer_id > 0 else config.num_channels\n",
        "            self.out_conv_dim = config.conv_dim[layer_id]\n",
        "\n",
        "            self.conv = nn.Conv1d(\n",
        "                self.in_conv_dim,\n",
        "                self.out_conv_dim,\n",
        "                kernel_size=config.conv_kernel[layer_id],\n",
        "                stride=config.conv_stride[layer_id],\n",
        "                bias=config.conv_bias,\n",
        "                padding=config.conv_kernel[layer_id]//2,\n",
        "            )\n",
        "            self.activation = ACT2FN[config.feat_extract_activation]\n",
        "            self.layer_norm = nn.LayerNorm(\n",
        "                self.out_conv_dim, elementwise_affine=True)\n",
        "\n",
        "    class Wav2Vec2NoLayerNormConvLayer(Wav2Vec2NoLayerNormConvLayer):\n",
        "        def __init__(self, config, layer_id=0):\n",
        "            super().__init__(config, layer_id)\n",
        "\n",
        "            self.in_conv_dim = config.conv_dim[layer_id -\n",
        "                                               1] if layer_id > 0 else config.num_channels\n",
        "            self.out_conv_dim = config.conv_dim[layer_id]\n",
        "\n",
        "            self.conv = nn.Conv1d(\n",
        "                self.in_conv_dim,\n",
        "                self.out_conv_dim,\n",
        "                kernel_size=config.conv_kernel[layer_id],\n",
        "                stride=config.conv_stride[layer_id],\n",
        "                bias=config.conv_bias,\n",
        "                padding=config.conv_kernel[layer_id]//2,\n",
        "            )\n",
        "            self.activation = ACT2FN[config.feat_extract_activation]\n",
        "\n",
        "    class Wav2Vec2GroupNormConvLayer(Wav2Vec2GroupNormConvLayer):\n",
        "        def __init__(self, config, layer_id=0):\n",
        "            super().__init__(config, layer_id)\n",
        "\n",
        "            self.in_conv_dim = config.conv_dim[layer_id -\n",
        "                                               1] if layer_id > 0 else config.num_channels\n",
        "            self.out_conv_dim = config.conv_dim[layer_id]\n",
        "\n",
        "            self.conv = nn.Conv1d(\n",
        "                self.in_conv_dim,\n",
        "                self.out_conv_dim,\n",
        "                kernel_size=config.conv_kernel[layer_id],\n",
        "                stride=config.conv_stride[layer_id],\n",
        "                bias=config.conv_bias,\n",
        "                padding=config.conv_kernel[layer_id]//2,\n",
        "            )\n",
        "\n",
        "            self.activation = ACT2FN[config.feat_extract_activation]\n",
        "            self.layer_norm = nn.GroupNorm(\n",
        "                num_groups=self.out_conv_dim, num_channels=self.out_conv_dim, affine=True)\n",
        "\n",
        "    class Wav2Vec2FeatureEncoder(Wav2Vec2FeatureEncoder):\n",
        "\n",
        "        def __init__(self, config):\n",
        "            super().__init__(config)\n",
        "\n",
        "            if config.feat_extract_norm == \"group\":\n",
        "                conv_layers = [Wav2Vec2GroupNormConvLayer(config, layer_id=0)] + [\n",
        "                    Wav2Vec2NoLayerNormConvLayer(config, layer_id=i + 1) for i in range(config.num_feat_extract_layers - 1)\n",
        "                ]\n",
        "            elif config.feat_extract_norm == \"layer\":\n",
        "                conv_layers = [\n",
        "                    Wav2Vec2LayerNormConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)\n",
        "                ]\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f\"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']\"\n",
        "                )\n",
        "\n",
        "            self.conv_layers = nn.ModuleList(conv_layers)\n",
        "            self.gradient_checkpointing = False\n",
        "            self._requires_grad = True\n",
        "\n",
        "        def forward(self, input_values):\n",
        "            hidden_states = input_values\n",
        "\n",
        "            # make sure hidden_states require grad for gradient_checkpointing\n",
        "            if self._requires_grad and self.training:\n",
        "                hidden_states.requires_grad = True\n",
        "\n",
        "            for conv_layer in self.conv_layers:\n",
        "                if self._requires_grad and self.gradient_checkpointing and self.training:\n",
        "                    hidden_states = self._gradient_checkpointing_func(\n",
        "                        conv_layer.__call__,\n",
        "                        hidden_states,\n",
        "                    )\n",
        "                else:\n",
        "                    hidden_states = conv_layer(hidden_states)\n",
        "\n",
        "            return hidden_states\n",
        "\n",
        "    class Wav2Vec2NoQuantizer(Wav2Vec2GumbelVectorQuantizer):\n",
        "        def __init__(self, config):\n",
        "            super().__init__(config)\n",
        "            self.proj = nn.Linear(config.conv_dim[-1], config.codevector_dim)\n",
        "\n",
        "        @staticmethod\n",
        "        def _compute_perplexity(probs, mask=None):\n",
        "            # should probably make this a tensor\n",
        "            return 0\n",
        "\n",
        "        def forward(self, hidden_states, mask_time_indices=None):\n",
        "            hidden_states = self.proj(hidden_states)\n",
        "            perplexity = self._compute_perplexity(\n",
        "                hidden_states,  mask_time_indices)\n",
        "\n",
        "            return hidden_states, perplexity\n",
        "\n",
        "    model.wav2vec2.feature_extractor = Wav2Vec2FeatureEncoder(configuration)\n",
        "\n",
        "    if remove_quantization:\n",
        "        model.wav2vec2.quantizer = Wav2Vec2NoQuantizer(configuration)\n",
        "        model.config.diversity_loss_weight = 0.0\n",
        "\n",
        "    def list_files_with_extension(root_folder, extension):\n",
        "        \"\"\"\n",
        "        List all files with a specific extension in a folder and its subfolders.\n",
        "\n",
        "        Parameters:\n",
        "        - root_folder (str): The root folder to start the search.\n",
        "        - extension (str): The file extension to look for (e.g., '.txt').\n",
        "\n",
        "        Returns:\n",
        "        - List of file paths with the specified extension.\n",
        "        \"\"\"\n",
        "        matching_files = []\n",
        "        walk_list = list(os.walk(root_folder))\n",
        "        sorted_walk_list = sorted(walk_list, key=lambda x: x[0])\n",
        "\n",
        "        # Traverse the directory tree\n",
        "        for dirpath, dirnames, filenames in sorted_walk_list:\n",
        "            for filename in filenames:\n",
        "                if filename.endswith(extension):\n",
        "                    full_path = os.path.join(dirpath, filename)\n",
        "                    matching_files.append(full_path)\n",
        "\n",
        "        return matching_files\n",
        "\n",
        "    # im using dataset class from shared file\n",
        "    class EEGDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, path, extension):\n",
        "            self.path = path\n",
        "            self.extension = extension\n",
        "            self.items = list_files_with_extension(self.path, self.extension)\n",
        "            # self.items = self.filter_valid_files(all_items)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.items)\n",
        "\n",
        "        def get_filename(self, idx):\n",
        "            return self.items[idx]\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            file_id = self.items[idx]\n",
        "            raw_file = mne.io.read_raw_fif(file_id, preload=True)\n",
        "            data = raw_file.get_data()\n",
        "            # model expects dictionary with at least this keyword\n",
        "            return {\"input_values\": torch.Tensor(data)}\n",
        "\n",
        "    mne.set_log_level(\"ERROR\")\n",
        "\n",
        "    train_dataset = EEGDataset(processed_folder, \".fif\")\n",
        "\n",
        "    ep = train_dataset[0]\n",
        "    num_samples = ep[\"input_values\"].shape[1]\n",
        "\n",
        "    for i, s in enumerate(configuration.conv_stride):\n",
        "        if s > 1:\n",
        "            num_samples = (num_samples + 2*(configuration.conv_kernel[i]//2) - (\n",
        "                configuration.conv_kernel[i]-1) - 1)//s + 1\n",
        "\n",
        "    set_seed(seed)\n",
        "\n",
        "    accelerator = Accelerator(kwargs_handlers=[DistributedDataParallelKwargs(find_unused_parameters=True)])\n",
        "    # accelerator.print(\"Started\")\n",
        "    if accelerator.is_main_process:\n",
        "      run = wandb.init(\n",
        "          project = \"multigpu_test_kaggle\",\n",
        "          config = {\n",
        "              \"lr\": learning_rate,\n",
        "              \"bs\": bs,\n",
        "              \"num_epochs\": num_epochs\n",
        "          }\n",
        "      )\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "    # progress_bar = tqdm(train_dataloader, disable=not accelerator.is_main_process)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=learning_rate, epochs=num_epochs, steps_per_epoch=len(train_dataloader), pct_start=warmup_ratio)\n",
        "\n",
        "    model, optimizer, train_dataloader, scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, scheduler)\n",
        "    # accelerator.print(\"prepare done\")\n",
        "    steps = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch in tqdm(train_dataloader):\n",
        "            steps += 1\n",
        "            bs_data, cin, seq_length = batch[\"input_values\"].shape\n",
        "            seq_length = int(num_samples)\n",
        "            mask_time_indices = _compute_mask_indices(shape=(\n",
        "                bs_data, seq_length), mask_prob=configuration.mask_time_prob, mask_length=configuration.mask_time_length)\n",
        "            sampled_negative_indices = _sample_negative_indices(features_shape=(\n",
        "                bs_data,  seq_length), num_negatives=configuration.num_negatives, mask_time_indices=mask_time_indices)\n",
        "\n",
        "            mask_time_indices = torch.tensor(\n",
        "                data=mask_time_indices, device=accelerator.device, dtype=torch.long)\n",
        "            sampled_negative_indices = torch.tensor(\n",
        "                data=sampled_negative_indices, device=accelerator.device, dtype=torch.long)\n",
        "\n",
        "            # accelerator.print(\"before model forward \")\n",
        "\n",
        "            outputs = model(input_values=batch[\"input_values\"], mask_time_indices=mask_time_indices,\n",
        "                            sampled_negative_indices=sampled_negative_indices, return_dict=True)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # accelerator.print(\"shape of loss \", loss)\n",
        "            # accelerator.print(\"loss mean \", loss.mean())\n",
        "            gathered_loss = accelerator.gather(loss)\n",
        "            # accelerator.print(\"shape of gathered loss \", gathered_loss)\n",
        "\n",
        "            if accelerator.is_main_process:\n",
        "                wandb.log({\"loss\": gathered_loss.mean().item()})\n",
        "\n",
        "            if steps % logging_steps == 0:\n",
        "                accelerator.print(\n",
        "                    f\"Doing step {steps}, loss: %.2f\" % (gathered_loss.mean()))\n",
        "            if steps % save_steps == 0:\n",
        "                accelerator.wait_for_everyone()\n",
        "                unwrapped_model = accelerator.unwrap_model(model)\n",
        "                accelerator.save_model(\n",
        "                    unwrapped_model, f\"{output_dir}/model_{steps}\")\n",
        "                del unwrapped_model\n",
        "\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "    if accelerator.is_main_process:\n",
        "      wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6a93061d-54d0-4362-b18c-e300e0d79ec5",
        "_uuid": "723bc320-0a98-4bca-8bd7-5b1f008763a9",
        "execution": {
          "iopub.execute_input": "2024-07-06T13:07:02.238893Z",
          "iopub.status.busy": "2024-07-06T13:07:02.238506Z",
          "iopub.status.idle": "2024-07-06T13:07:05.217998Z",
          "shell.execute_reply": "2024-07-06T13:07:05.217060Z",
          "shell.execute_reply.started": "2024-07-06T13:07:02.238862Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "NdltT21vrP2D"
      },
      "outputs": [],
      "source": [
        "from accelerate import notebook_launcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e227e275-3dcd-4bfe-bafb-39031a99ec96",
        "_uuid": "bc2e6d3f-70b3-4949-a41b-a69662223ec4",
        "execution": {
          "iopub.execute_input": "2024-07-06T13:07:06.199614Z",
          "iopub.status.busy": "2024-07-06T13:07:06.199122Z",
          "iopub.status.idle": "2024-07-06T13:07:06.204256Z",
          "shell.execute_reply": "2024-07-06T13:07:06.203159Z",
          "shell.execute_reply.started": "2024-07-06T13:07:06.199571Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "ChOIaKsmrP2F"
      },
      "outputs": [],
      "source": [
        "#Pass number of epochs, batch size and seed here. In order.\n",
        "args = (5, 2, 42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_launcher(main, args, num_processes = 1)"
      ],
      "metadata": {
        "id": "q-jGb4Iw8G1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUJp2O8T0XrQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 5313848,
          "sourceId": 8831361,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}